{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a501b71-7efc-4d93-a468-12a8dfe73b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il dataset\n",
    "data = pd.read_csv('C:\\\\Users\\\\CRAIA-AREA EDUCATORI\\\\Documents\\\\Università\\\\Machine Learning\\\\beer_data\\\\beer_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9897359c-c202-4a02-a2ea-bb0bab150510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4796e50b-a709-4241-a56c-7d26c11a8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_category\n",
      "Medio    1050225\n",
      "Alto      415705\n",
      "Basso     120684\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creazione della colonna categoriale basata su review_overall\n",
    "def categorize_overall(score):\n",
    "    if score <= 2.5:\n",
    "        return 'Basso'\n",
    "    elif score <= 4:\n",
    "        return 'Medio'\n",
    "    else:\n",
    "        return 'Alto'\n",
    "\n",
    "data['overall_category'] = data['review_overall'].apply(categorize_overall)\n",
    "\n",
    "# Controlliamo la distribuzione delle classi\n",
    "print(data['overall_category'].value_counts())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#vediamo che la distribuzione delle classi è sbilanciata: la classe \"Medio\" ha molte più istanze rispetto alle altre due, mentre \"Basso\" è la più rara.",
   "id": "aa21d25bc28fd514"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbc513b-954a-4589-8a7b-d660fb1047f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione delle classi nel training set: [333017  96648 839626]\n",
      "Distribuzione delle classi nel test set: [ 82688  24036 210599]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Definiamo X (feature) e y (target)\n",
    "X = data[['review_aroma', 'review_appearance', 'review_palate', 'review_taste']]  # Le feature numeriche\n",
    "y = data['overall_category']\n",
    "\n",
    "# Convertiamo il target in valori numerici\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Suddividiamo in training e test set (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Controlliamo la distribuzione delle classi nel training set\n",
    "print(\"Distribuzione delle classi nel training set:\", np.bincount(y_train))\n",
    "print(\"Distribuzione delle classi nel test set:\", np.bincount(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745e1b90-920f-4aa3-b126-86792980344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardizziamo le feature per migliorare la performance dei modelli.\n",
    "# Applichiamo lo standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16464d76-d349-44dd-aea4-a145516af5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alto       0.70      0.53      0.61     82688\n",
      "       Basso       0.78      0.62      0.69     24036\n",
      "       Medio       0.80      0.89      0.84    210599\n",
      "\n",
      "    accuracy                           0.78    317323\n",
      "   macro avg       0.76      0.68      0.71    317323\n",
      "weighted avg       0.77      0.78      0.77    317323\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 44210    121  38357]\n",
      " [    77  14833   9126]\n",
      " [ 18612   3956 188031]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Addestramento del modello\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predizioni sul test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Class \"Alto\":\n",
    "\n",
    "#Precision: 70% - Il modello classifica correttamente il 70% dei casi predetti come \"Alto\".\n",
    "#Recall: 53% - Su tutti i casi effettivamente \"Alto\", il modello ne classifica correttamente solo il 53%.\n",
    "#F1-score: 61% - Mediocre, probabilmente a causa del basso recall.\n",
    "#Class \"Basso\":\n",
    "\n",
    "#Precision: 78% - Buona capacità di classificare correttamente i casi predetti come \"Basso\".\n",
    "#Recall: 62% - Potrebbe essere migliorato, dato che il 38% dei casi \"Basso\" effettivi non viene riconosciuto.\n",
    "#F1-score: 69% - Complessivamente buono.\n",
    "#Class \"Medio\":\n",
    "\n",
    "#Precision: 80%, Recall: 89%, F1-score: 84% - Ottima performance per questa classe dominante. Tuttavia, ciò potrebbe dipendere dal fatto che è la classe maggioritaria e quindi il modello tende a favorirla.\n",
    "#Accuracy: 78% - Non male, ma è influenzata dallo sbilanciamento delle classi.\n",
    "\n",
    "#Macro Average (media non pesata sulle classi): Precision (76%), Recall (68%), F1-score (71%) - Indica che il modello potrebbe migliorare sulle classi meno rappresentate.\n",
    "\n",
    "#Weighted Average (pesata in base alla distribuzione delle classi): Precision (77%), Recall (78%), F1-score (77%) - Indicatore complessivo delle performance del modello.\n",
    "\n",
    "#Confusion Matrix\n",
    "#La matrice di confusione rivela i principali problemi:\n",
    "#\"Alto\" viene spesso confuso con \"Medio\" (38,357 errori).\n",
    "#\"Basso\" è abbastanza distinto, ma una parte significativa (9,126 errori) viene scambiata per \"Medio\".\n",
    "#La classe \"Medio\" ha la migliore performance, ma ci sono ancora 18,612 casi classificati erroneamente come \"Alto\"."
   ],
   "id": "f606d5b7214b57d9"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5ed056-125c-45e6-ac9c-57246877ea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Weighted Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alto       0.58      0.74      0.65     82688\n",
      "       Basso       0.46      0.89      0.60     24036\n",
      "       Medio       0.86      0.67      0.75    210599\n",
      "\n",
      "    accuracy                           0.71    317323\n",
      "   macro avg       0.63      0.77      0.67    317323\n",
      "weighted avg       0.76      0.71      0.72    317323\n",
      "\n",
      "Confusion Matrix (Weighted Logistic Regression):\n",
      "[[ 61182    520  20986]\n",
      " [   170  21292   2574]\n",
      " [ 43866  24908 141825]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression con class weights\n",
    "logistic_weighted = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "logistic_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Predizioni sul test set\n",
    "y_pred_weighted = logistic_weighted.predict(X_test)\n",
    "\n",
    "# Valutazione\n",
    "print(\"Classification Report (Weighted Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_weighted, target_names=le.classes_))\n",
    "\n",
    "print(\"Confusion Matrix (Weighted Logistic Regression):\")\n",
    "print(confusion_matrix(y_test, y_pred_weighted))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Aspetto\t                        Logistic Regression Standard\t     Logistic Regression con class_weight='balanced'\n",
    "#Gestione delle classi\t        Ignora lo squilibrio tra le classi.\tPesa automaticamente le classi in base alla loro frequenza.\n",
    "#Bias verso le classi\t        Favorisce la classe maggioritaria.\tBilancia il peso delle classi per migliorare le predizioni sulle classi                                                                    minoritarie.\n",
    "#Uso\t                        Dataset equilibrato.\t            Dataset sbilanciato.\n",
    "#Prestazioni su classi minori\tPiù basse (bias).\t                Più alte (bilancia il peso)."
   ],
   "id": "ee228e2842cd7a69"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405165ff-1c43-4b61-9928-b1f8145db52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alto       0.70      0.54      0.61     82688\n",
      "       Basso       0.77      0.64      0.70     24036\n",
      "       Medio       0.80      0.89      0.84    210599\n",
      "\n",
      "    accuracy                           0.78    317323\n",
      "   macro avg       0.76      0.69      0.72    317323\n",
      "weighted avg       0.77      0.78      0.77    317323\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      "[[ 44533    141  38014]\n",
      " [    89  15290   8657]\n",
      " [ 18799   4452 187348]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creazione e addestramento del modello Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predizioni sul test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Valutazione\n",
    "print(\"Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
    "\n",
    "print(\"Confusion Matrix (Random Forest):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Aspetto\t                                  Logistic Regression\t                             Random Forest\n",
    "#Tipo di modello    \t                Modello lineare (usa un'equazione lineare \t        Modello non lineare (ensemble                                                         per separare le classi).                            di alberi decisionali).\n",
    "#Decision Boundary\t                    Lineare (o quasi-lineare, a seconda del kernel\t    Non lineare (più flessibile per                                                       o delle feature).                                   pattern complessi).\n",
    "#Robustezza al rumore\t                Sensibile al rumore nelle feature.\t                Più robusto, grazie alla votazione degli alberi.\n",
    "#Gestione delle feature\t                Richiede dati normalizzati e indipendenza\t        Non richiede normalizzazione; gestisce                                                tra le feature.                                     feature correlate.\n",
    "#Sbilanciamento delle classi\t        Può essere gestito con class_weight.\t            Può essere gestito con class_weight o                                                                                                     modificando i dati.\n",
    "#Prestazioni su dataset complessi\t    Può sottoperformare se la separazione\t            Funziona bene con dataset complessi                                                   tra classi è complessa o non lineare.               e feature non lineari.\n",
    "#Overfitting\t                        Meno propenso all'overfitting \t                    Può overfittare, ma mitigato grazie al                                                (modello più semplice).                             bagging."
   ],
   "id": "632921500f4e6f17"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aeca524-d477-49f7-9b42-5082ab1b35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migliori parametri trovati:\n",
      "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 10, 'class_weight': 'balanced'}\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.0min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.7min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.6min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 3.1min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 3.8min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 3.8min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.6min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 3.5min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 3.2min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 3.2min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 3.5min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 3.4min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 3.3min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 3.6min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 3.6min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 3.7min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 3.7min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 9.1min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=10.1min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 9.3min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 3.6min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 3.6min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 3.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 3.4min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 3.3min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.6min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 3.2min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 2.9min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 7.0min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 7.6min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 7.4min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 7.6min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.6min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 2.8min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 2.0min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time= 1.8min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.6min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 2.9min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 2.8min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.9min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 3.2min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 3.1min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 2.8min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 3.1min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.8min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.4min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.4min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.3min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 2.9min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 7.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 7.5min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 7.4min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 3.1min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 3.1min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.3min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time= 1.3min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 2.9min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 6.8min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 6.8min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 7.2min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 3.0min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time= 2.9min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 7.4min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 7.4min\n",
      "[CV] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 2.8min\n",
      "[CV] END class_weight=balanced, max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time= 2.7min\n",
      "[CV] END class_weight=balanced, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time= 1.1min\n"
     ]
    }
   ],
   "source": [
    "# Utilizzo Random Search per andare ad ottimizzare i parametri della random forest\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definizione della griglia ridotta\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # Numero di combinazioni casuali da testare\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Avvio della ricerca\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Migliori parametri trovati\n",
    "print(\"Migliori parametri trovati:\")\n",
    "print(random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317c8872-af90-4f2e-886b-f9510920e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Miglior RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.78      0.65     82688\n",
      "           1       0.52      0.87      0.65     24036\n",
      "           2       0.87      0.66      0.75    210599\n",
      "\n",
      "    accuracy                           0.71    317323\n",
      "   macro avg       0.65      0.77      0.68    317323\n",
      "weighted avg       0.76      0.71      0.72    317323\n",
      "\n",
      "\n",
      "Confusion Matrix (Miglior RF):\n",
      "[[ 64677    560  17451]\n",
      " [   205  20890   2941]\n",
      " [ 51662  19062 139875]]\n"
     ]
    }
   ],
   "source": [
    "# Addestramento del modello finale\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Valutazione sul test set\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "\n",
    "# Metriche di valutazione\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Classification Report (Miglior RF):\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"\\nConfusion Matrix (Miglior RF):\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#  il modello ottimizzato con RandomizedSearchCV migliora le prestazioni per le classi meno rappresentate, ma sacrifica un po' di accuratezza generale.",
   "id": "2e30caaca73b932d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849dd208-6c08-4dd2-b7f6-d6be400a9f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26dc77a9-3935-4a1c-aa4d-7212934e27f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione delle classi prima di SMOTE: 2    839626\n",
      "0    333017\n",
      "1     96648\n",
      "Name: count, dtype: int64\n",
      "Distribuzione delle classi dopo SMOTE: 0    839626\n",
      "2    839626\n",
      "1    839626\n",
      "Name: count, dtype: int64\n",
      "Classification Report (RF con SMOTE):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.78      0.65     82688\n",
      "           1       0.50      0.87      0.64     24036\n",
      "           2       0.87      0.66      0.75    210599\n",
      "\n",
      "    accuracy                           0.71    317323\n",
      "   macro avg       0.64      0.77      0.68    317323\n",
      "weighted avg       0.76      0.71      0.72    317323\n",
      "\n",
      "\n",
      "Confusion Matrix (RF con SMOTE):\n",
      "[[ 64502    563  17623]\n",
      " [   204  21008   2824]\n",
      " [ 51257  20099 139243]]\n"
     ]
    }
   ],
   "source": [
    "# Installazione del pacchetto necessario (se non già installato)\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Applichiamo SMOTE al set di addestramento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Controlla le dimensioni dopo il resampling\n",
    "print(\"Distribuzione delle classi prima di SMOTE:\", pd.Series(y_train).value_counts())\n",
    "print(\"Distribuzione delle classi dopo SMOTE:\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Addestramento del modello con i dati bilanciati\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_depth=10,\n",
    "    class_weight=None,  # Rimuoviamo il bilanciamento interno per evitare ridondanza\n",
    "    random_state=42\n",
    ")\n",
    "best_rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Valutazione sul test set\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "\n",
    "# Metriche di valutazione\n",
    "print(\"Classification Report (RF con SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"\\nConfusion Matrix (RF con SMOTE):\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T10:46:00.461131Z",
     "start_time": "2025-01-19T10:46:00.442039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vado ad utilizzare SMOTE che mi va a bilanciare le tre classi per analizzare se così facendo si hanno dei miglioramenti nel modello.\n",
    "# Tuttavia com'è possibile vedere le prestazioni del modello migliorano ma non in modo significativo"
   ],
   "id": "1a0b926681d7681a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4c49e-8d81-4caf-a615-c041cb438a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
